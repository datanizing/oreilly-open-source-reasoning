{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "956bfaf4-8a59-42d4-af39-650d05c13cd8",
   "metadata": {},
   "source": [
    "# Running Qwen3-8B on MacOS (Apple Silicon)\n",
    "\n",
    "The previous notebooks were optimized for GPUs (using CUDA). If you want to run models on your Mac,\n",
    "that hardware is also well-suited for reasoning LLMs.\n",
    "\n",
    "Apple developed their own framework ([mlx](https://github.com/ml-explore/mlx)). This is a very\n",
    "nice framework which can also be accessed via Python.\n",
    "\n",
    "Apple hardware is particularly well-suited for running LLMs as the memory bandwidth is higher\n",
    "compared to PCs.  `mlx` is optimized for Apple Silicon and allows running models very quickly,\n",
    "especially if you use quantized models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cc2672-01e3-4f6f-8143-e7f5a8c305ac",
   "metadata": {},
   "source": [
    "The interface is completely different from `transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0b530-e1e0-44c4-9994-5235941ccac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb18789-f834-4c8a-b7e3-c565435cdc24",
   "metadata": {},
   "source": [
    "Quite comfortabel: `model` and `tokenizer` are returned as a pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd995b0-a4a9-423a-8633-6c35ddc46979",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load(\"mlx-community/DeepSeek-R1-0528-Qwen3-8B-4bit-DWQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab23dcd-c55a-4b19-add4-055a33e30551",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How many 'r's are in 'strawberry'?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81e0aca-d84e-4ddb-8b09-69e74fb72cc9",
   "metadata": {},
   "source": [
    "`apply_chat_template` also exists in the `tokenizer`, similar to Hugging Face `transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8b2185-a750-49e8-bf53-170d79f2f500",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.chat_template is not None:\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eb92a1-98f3-49e0-9477-3b6a29dd4bf0",
   "metadata": {},
   "source": [
    "Generation works also in a very similar way, but streaming output is directly shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9789d19-2cf5-4e9b-9f8c-3f08888a9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model, tokenizer, prompt=prompt, verbose=True, max_tokens=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81ad2d1-369b-4c8f-a9c2-07afeaa795cf",
   "metadata": {},
   "source": [
    "The response variable contains the whole string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaade4bd-1db1-4dfd-a634-73aa5b633986",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc7283-d1e2-45d3-bca0-beb54b2df4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23390ae4-cf93-4a58-a5dc-b056ae9bb37e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning",
   "language": "python",
   "name": "reasoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
