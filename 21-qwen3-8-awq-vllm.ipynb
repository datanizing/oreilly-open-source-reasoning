{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f21b6-2787-4096-827d-04f68d4bb7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366c242-39a2-4061-91f3-c057301f7239",
   "metadata": {},
   "source": [
    "https://github.com/vllm-project/vllm/issues/13127\n",
    "\n",
    "`vllm` does not yet support `transformers` 5.0. You can install an older version of `transformers`.\n",
    "For simplicity, I used a different kernel here with an old version of the library installed.\n",
    "As this notebook is just for demonstration how you can save RAM (and increase the speed),\n",
    "you do not necessarily have to run it!\n",
    "\n",
    "Unfortunately, the *monkey patch* did not work for me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ef4b6-cf16-4848-9b34-9a03e2c60def",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-8B-AWQ\"\n",
    "llm = LLM(model=model_name, max_model_len=16384, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5538d476-9299-491c-ad42-5fdb35036b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb375e-e656-4c9a-89e1-d497038303fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many 'r's are in 'strawberry'?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03cee0-f027-4a81-9017-8b61b201fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "  max_tokens=1024,\n",
    "  temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc696607-2b0c-40c3-bb99-2999ed329049",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "output = llm.chat(messages=messages, sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bda579-e821-43a0-8854-f4ce2b72528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in output:\n",
    "    prompt = o.prompt\n",
    "    generated_text = o.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe3cdb-c846-4ccb-a986-e7f4a3f562f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726b6a8-9c62-40af-a8cc-5a797aaf4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(output[0].outputs[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148fe03a-85b0-4c50-bf35-81566af4fd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollm",
   "language": "python",
   "name": "ollm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
