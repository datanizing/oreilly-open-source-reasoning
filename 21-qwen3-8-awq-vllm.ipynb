{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8916d4b8-7f2b-470a-bf25-c3934beae2df",
   "metadata": {},
   "source": [
    "# Runing a quantized Qwen3-8B model\n",
    "\n",
    "As you have seen in the last notebook, the speed of the models could be tremendously increased\n",
    "by reducing the number of (active) parameters. In model inference, the speed is dominated by\n",
    "the availabe memory bandwidth. This is the main reason why GPUs are so much faster in \n",
    "text generation compared to CPUs (in addition to prompt parsing).\n",
    "\n",
    "However, if we can reduce the size of the parameters (not just the number), we could also get\n",
    "speed increases. This can be achieved by quantization. Unfortunately, `transformers` do not\n",
    "support quantized models. Therefore, we have to use another software which is optimized for\n",
    "that.\n",
    "\n",
    "`vllm` can be used both as a library and as an Open AI compatible REST server. In this\n",
    "notebook, we take the library approach, but it is quite easy to switch later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f21b6-2787-4096-827d-04f68d4bb7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366c242-39a2-4061-91f3-c057301f7239",
   "metadata": {},
   "source": [
    "https://github.com/vllm-project/vllm/issues/13127\n",
    "\n",
    "`vllm` does not yet support `transformers` 5.0. You can install an older version of `transformers`.\n",
    "For simplicity, I used a different kernel here with an old version of the library installed.\n",
    "As this notebook is just for demonstration how you can save RAM (and increase the speed),\n",
    "you do not necessarily have to run it!\n",
    "\n",
    "Unfortunately, the *monkey patch* did not work for me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ef4b6-cf16-4848-9b34-9a03e2c60def",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-8B-AWQ\"\n",
    "llm = LLM(model=model_name, max_model_len=16384, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5538d476-9299-491c-ad42-5fdb35036b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb375e-e656-4c9a-89e1-d497038303fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How many 'r's are in 'strawberry'?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03cee0-f027-4a81-9017-8b61b201fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "  max_tokens=1024,\n",
    "  temperature=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff2c66c-2f2c-4276-93cb-d96d633fd209",
   "metadata": {},
   "source": [
    "Note that this is (hopefully) much faster than the generation based on `transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc696607-2b0c-40c3-bb99-2999ed329049",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "output = llm.chat(messages=messages, sampling_params=sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bda579-e821-43a0-8854-f4ce2b72528b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in output:\n",
    "    prompt = o.prompt\n",
    "    generated_text = o.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe3cdb-c846-4ccb-a986-e7f4a3f562f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726b6a8-9c62-40af-a8cc-5a797aaf4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(output[0].outputs[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148fe03a-85b0-4c50-bf35-81566af4fd0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollm",
   "language": "python",
   "name": "ollm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
