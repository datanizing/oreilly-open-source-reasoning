{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5b9a09-9a03-45d8-aa59-b6d4fe0cc300",
   "metadata": {},
   "source": [
    "# Run a \"small\" reasoning model with tool calling\n",
    "\n",
    "In the previous notebook, you have already become familiar with Nanbeige.\n",
    "\n",
    "As reasoning models are often used in agentic scenarios (as their conclusions are much\n",
    "more likely to be true), tool calling can extend this functionality and avoid loops.\n",
    "\n",
    "This notebook shows how we can tell the model that a tool should be called. We don't\n",
    "perform the actual call, but it would be easy to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f21b6-2787-4096-827d-04f68d4bb7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ef4b6-cf16-4848-9b34-9a03e2c60def",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Nanbeige/Nanbeige4.1-3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf03cee0-f027-4a81-9017-8b61b201fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    use_fast=False,\n",
    "    trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc696607-2b0c-40c3-bb99-2999ed329049",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe3cdb-c846-4ccb-a986-e7f4a3f562f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'user',  'content': 'I want to find a course about reasoning language models!'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c961cf95-c2d2-4a0d-8267-5ce9cdef5e6e",
   "metadata": {},
   "source": [
    "This is a list of tools which are available to the LLM. The format does not depend on the\n",
    "actual LLM, but is more or less standardized.\n",
    "\n",
    "Don't be afraid, even if the mentioned function exists, the LLM won't automatically call\n",
    "the function. Rather, it finds out (by reasoning) whether the function could help in\n",
    "solving the problem. If yes, it suggest calling the function *including its arguments*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdf27f7-d00e-438d-a2e6-c3f94532e4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{'type': 'function',\n",
    "  'function': {'name': 'SearchOReillyCourses',\n",
    "   'description': 'Find O\\'Reilly courses about a certain topic.',\n",
    "   'parameters': {'type': 'dict',\n",
    "    'properties': {'topic': {'type': 'string',\n",
    "      'description': 'A course topic for searching O\\'Reilly\\'s online library.'},\n",
    "    'required': ['topic']}}}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a6bc5-d836-4a40-a690-8228014ddd26",
   "metadata": {},
   "source": [
    "The actual function call is what you have to perform on your own. Therefore, a suitable\n",
    "output format is crucial in this case. This is why we add `return_dict=True` which\n",
    "makes parsing a suggested function call much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0b37e8-0a7d-4bc0-a12e-cda170d6717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tools,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    enable_thinking=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafae2aa-c248-4a74-8bea-d1d9b8ba6c52",
   "metadata": {},
   "source": [
    "Note the Chinese system prompt, which we could change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e45cc-0488-44b8-8f04-af38543b4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37931c76-faab-40d0-82d7-80dad7744c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74303e-51c0-44cd-bd6f-706dc1648eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f112da3-983e-46c8-957e-ccca09033483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only read output, skip input\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2708605-b263-4b28-9375-7b4bb3eb6573",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad78ff5f-e19c-4401-859b-6e6d8ce37ba1",
   "metadata": {},
   "source": [
    "Within the `<tool_call>` container you can see what the LLM suggested:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d5f34-cfda-4ecc-9b6e-f8e36ff6ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058eb09e-5b31-4679-ab82-3538ded97fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(tokenizer.decode(output_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a85b5c-01ae-4dd1-b4c6-b7d9984587b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reasoning",
   "language": "python",
   "name": "reasoning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
